{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IUxLVIghylFO",
        "rngHDVmLyvbA",
        "Pp4_juFYzEFn",
        "eeWV_rnx0L4B",
        "w_hCAEuB1Au8",
        "4IMIVpGm2j0w"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 : Set up colab gpu runtime environment"
      ],
      "metadata": {
        "id": "IUxLVIghylFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch\n",
        "!pip install -U git+https://github.com/albumentations-team/albumentations\n",
        "!pip install --upgrade opencv-contrib-python"
      ],
      "metadata": {
        "id": "IW2niYwjymSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset\n",
        "\n",
        "original author of the dataset :\n",
        "\n",
        "https://github.com/VikramShenoy97/Human-Segmentation-Dataset"
      ],
      "metadata": {
        "id": "rngHDVmLyvbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/parth1620/Human-Segmentation-Dataset-master.git"
      ],
      "metadata": {
        "id": "kCHb0zYCyxPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries "
      ],
      "metadata": {
        "id": "Pp4_juFYzEFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/Human-Segmentation-Dataset-master')"
      ],
      "metadata": {
        "id": "H1Ctnt3izGBk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import cv2\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "import helper"
      ],
      "metadata": {
        "id": "77QlXAuXzK89"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Setup Configurations"
      ],
      "metadata": {
        "id": "-iWcJ0CvzOk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_FILE = '/content/Human-Segmentation-Dataset-master/train.csv' # Contains the images & corresponding masks\n",
        "DATA_DIR = '/content/'\n",
        "\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "EPOCHS = 25\n",
        "LR = 0.003\n",
        "IMAGE_SIZE = 320\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "ENCODER = 'timm-efficientnet-b0' \n",
        "WEIGHTS = 'imagenet'"
      ],
      "metadata": {
        "id": "ZAZzvndqzP9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(CSV_FILE)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "_TP4v2FUzo1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect data"
      ],
      "metadata": {
        "id": "eujbRsc0zweR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row = df.iloc[10]\n",
        "image_path = row.images\n",
        "mask_path = row.masks"
      ],
      "metadata": {
        "id": "HXq9tY0ZzpSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(image_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)/ 255.0"
      ],
      "metadata": {
        "id": "K6eGxbjzz2zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "        \n",
        "ax1.set_title('IMAGE')\n",
        "ax1.imshow(image)\n",
        "\n",
        "ax2.set_title('GROUND TRUTH')\n",
        "ax2.imshow(mask,cmap = 'gray')"
      ],
      "metadata": {
        "id": "sDuvA5rbz240"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the data**"
      ],
      "metadata": {
        "id": "cuteSEQ70EzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, valid_df =  train_test_split(df,test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "td0Gbljmz28R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PWsEmxqhz3AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation Functions\n",
        "\n",
        "For instance and semantic segmentation tasks, you need to augment both the input image and one or more output masks.\n",
        "\n",
        "Albumentations ensures that the input image and the output mask will receive the same set of augmentations with the same parameters.\n",
        "\n",
        "\n",
        "\n",
        "albumentation documentation : https://albumentations.ai/docs/"
      ],
      "metadata": {
        "id": "eeWV_rnx0L4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A"
      ],
      "metadata": {
        "id": "kXT-nZJvz3Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_augs():\n",
        "  return A.Compose([\n",
        "      A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
        "      A.HorizontalFlip(p=0.5),\n",
        "      A.VerticalFlip(p=0.5)\n",
        "  ],is_check_shapes = False)\n",
        "\n",
        "def get_valid_augs():\n",
        "  return A.Compose([\n",
        "      A.Resize(IMAGE_SIZE, IMAGE_SIZE)\n",
        "  ],is_check_shapes = False)"
      ],
      "metadata": {
        "id": "a8GKnQmCz3Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Custom Dataset**\n",
        "\n",
        "We need to create a pytorch dataset to load image & mask in pairs. "
      ],
      "metadata": {
        "id": "w_hCAEuB1Au8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "l_Q_laf106QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "\n",
        "  def __init__(self, df, augmentations):\n",
        "    self.df = df\n",
        "    self.augmentations = augmentations\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  #this returns the image & mask pairs according the the index\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    row = self.df.iloc[index]\n",
        "\n",
        "    image_path = row.images\n",
        "    mask_path = row.masks\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # (H,W,C)\n",
        "    \n",
        "\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) # (H, W)\n",
        "    mask = np.expand_dims(mask, axis = -1) # (H,W,C)\n",
        "\n",
        "    if self.augmentations:\n",
        "      #print(image.shape, mask.shape, image_path)\n",
        "      data = self.augmentations(image = image, mask = mask)\n",
        "      image= data['image']\n",
        "      mask = data['mask']\n",
        "      \n",
        "\n",
        "    #pytorch expects (C,H,W) we have in (H,W,C)\n",
        "    image = np.transpose(image, (2,0,1)).astype(np.float32)\n",
        "    mask = np.transpose(mask,(2,0,1)).astype(np.float32)\n",
        "\n",
        "    image = torch.Tensor(image)/255.0\n",
        "    mask = torch.round(torch.Tensor(mask)/255.0)\n",
        "\n",
        "    return image, mask"
      ],
      "metadata": {
        "id": "X55-TyvG06S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = SegmentationDataset(train_df, get_train_augs())\n",
        "validset =  SegmentationDataset(valid_df, get_valid_augs())"
      ],
      "metadata": {
        "id": "7-DXubdr06VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Size of Trainset : {len(trainset)}\")\n",
        "print(f\"Size of Validset : {len(validset)}\")"
      ],
      "metadata": {
        "id": "M5kJ2ZB51t8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "An utulity function to display image &  mask\n",
        "'''\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "import torch\n",
        "\n",
        "def show_image(image,mask,pred_image = None):\n",
        "    \n",
        "    if pred_image == None:\n",
        "        \n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "        \n",
        "        ax1.set_title('IMAGE')\n",
        "        ax1.imshow(image.permute(1,2,0).squeeze(),cmap = 'gray')\n",
        "        \n",
        "        ax2.set_title('GROUND TRUTH')\n",
        "        ax2.imshow(mask.permute(1,2,0).squeeze(),cmap = 'gray')\n",
        "        \n",
        "    elif pred_image != None :\n",
        "        \n",
        "        f, (ax1, ax2,ax3) = plt.subplots(1, 3, figsize=(10,5))\n",
        "        \n",
        "        ax1.set_title('IMAGE')\n",
        "        ax1.imshow(image.permute(1,2,0).squeeze(),cmap = 'gray')\n",
        "        \n",
        "        ax2.set_title('GROUND TRUTH')\n",
        "        ax2.imshow(mask.permute(1,2,0).squeeze(),cmap = 'gray')\n",
        "        \n",
        "        ax3.set_title('MODEL OUTPUT')\n",
        "        ax3.imshow(pred_image.permute(1,2,0).squeeze(),cmap = 'gray')\n",
        "        \n",
        "        \n"
      ],
      "metadata": {
        "id": "lz4_9msN2A2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 55\n",
        "image, mask = trainset[idx]\n",
        "show_image(image, mask) # helper.show_image(image, mask)"
      ],
      "metadata": {
        "id": "0BqNLU241uCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset into batches"
      ],
      "metadata": {
        "id": "4IMIVpGm2j0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "2JOVeJRg1uE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle=True)\n",
        "validloader = DataLoader(validset, batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "Ey24Ud102uzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of batches in tranloader : {len(trainloader)}')\n",
        "print(f'Number of batches in validloader : {len(validloader)}')"
      ],
      "metadata": {
        "id": "3NDhUFRc2u2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image,mask in trainloader:\n",
        "  break"
      ],
      "metadata": {
        "id": "FQMFdbW_2zfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Image batch shape {image.shape}')\n",
        "print(f'Mask batch shape {mask.shape}')"
      ],
      "metadata": {
        "id": "a1u2DgNb2ziH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Segmentation Model\n",
        "\n",
        "segmentation_models_pytorch documentation : https://smp.readthedocs.io/en/latest/"
      ],
      "metadata": {
        "id": "ThvsvPmV216G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import segmentation_models_pytorch as smp \n",
        "from segmentation_models_pytorch.losses import DiceLoss"
      ],
      "metadata": {
        "id": "a7j--4R623sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmenetationModel(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(SegmenetationModel, self).__init__()\n",
        "    self.arc = smp.Unet(\n",
        "        encoder_name= ENCODER,\n",
        "        encoder_weights=WEIGHTS,\n",
        "        in_channels=3, \n",
        "        classes= 1, \n",
        "        activation=None,\n",
        "    )\n",
        "\n",
        "  def forward(self,images, masks=None):\n",
        "    logits = self.arc(images)\n",
        "    \n",
        "\n",
        "    if masks !=None:\n",
        "      loss1 = DiceLoss(mode='binary')(logits,masks)\n",
        "      loss2 = nn.BCEWithLogitsLoss()(logits, masks)\n",
        "      return logits, loss1 + loss2\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "NJBTGm1C3GKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SegmenetationModel()\n",
        "model.to(DEVICE);"
      ],
      "metadata": {
        "id": "rBThwLrC3GSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Train and Validation Function"
      ],
      "metadata": {
        "id": "MqDxgpPT3WCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(dataloader, model, optimizer):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  for images, masks in tqdm(dataloader):\n",
        "    \n",
        "    images = images.to(DEVICE)    \n",
        "    masks = masks.to(DEVICE)   \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits , loss = model(images,masks)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss/len(dataloader)"
      ],
      "metadata": {
        "id": "6Wai4YYO3U5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_fn(dataloader, model, optimizer):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images, masks in tqdm(dataloader):\n",
        "      images = images.to(DEVICE)  \n",
        "      masks = masks.to(DEVICE)  \n",
        "\n",
        "      logits, loss = model(images, masks)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  return total_loss/len(dataloader)"
      ],
      "metadata": {
        "id": "ZjVF8MCk3GVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "HZlcc-D83gPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr= LR)"
      ],
      "metadata": {
        "id": "GRGGBDop3GaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = np.Inf\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "  train_loss = train_fn(trainloader,model, optimizer)\n",
        "  valid_loss = eval_fn(validloader,model,optimizer)\n",
        "\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(),'best_model.pt')\n",
        "    print('MODEL SAVED!!')\n",
        "  print(f'Epoch: {i+1} Train Loss:{train_loss} Valid Loss{valid_loss}')"
      ],
      "metadata": {
        "id": "CgqjcKSy3Ge-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "ljcY4mc23vwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 5\n",
        "model.load_state_dict(torch.load('/content/best_model.pt'))\n",
        "\n",
        "image, mask = validset[idx]\n",
        "logits_mask = model(image.to(DEVICE).unsqueeze(0)) # C,H, W -> (1,C,H,W) includes the batch dimension\n",
        "pred_mask = torch.sigmoid(logits_mask)\n",
        "pred_mask = (pred_mask > 0.5) *1.0 "
      ],
      "metadata": {
        "id": "f1gSc7d73Gi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image(image,mask,pred_mask.detach().cpu().squeeze(0))"
      ],
      "metadata": {
        "id": "nJN7dqE23zy7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}